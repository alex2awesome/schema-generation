{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data and formulate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters_level_1_2_clusters.csv  labeled_chunks__level-0.csv\n",
      "clusters_level_1_3_clusters.csv  labeled_chunks__level-1.csv\n",
      "clusters_level_2_4_clusters.csv  labeled_chunks__level-2.csv\n",
      "clusters_level_2_8_clusters.csv  labeled_chunks__level-3.csv\n",
      "clusters_level_3_5_clusters.csv  labeled_chunks__level-4.csv\n",
      "clusters_level_3_9_clusters.csv  labeled_chunks__level-5.csv\n",
      "clusters_level_4_10_clusters.csv labeled_chunks__level-6.csv\n",
      "clusters_level_4_12_clusters.csv labeled_hierarchical_tree.gml\n",
      "clusters_level_5_14_clusters.csv labeled_tree_visualization.png\n",
      "clusters_level_6_15_clusters.csv labels_and_descriptions.csv\n",
      "hierarchical_tree.gml            optimal_thresholds.csv\n",
      "inner_node_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sys \n",
    "sys.path.append('../src')\n",
    "from utils_openai_client import prompt_openai_model\n",
    "import utils_trees\n",
    "\n",
    "model_path = '../experiments/reasoning/qwq-32b/models/chess_agglomerative_clustering_outputs__discretized__labels_descriptions__child-nodes__output-labels-desc'\n",
    "! ls $model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tree = utils_trees.load_hierarchical_tree(f'{model_path}/labeled_hierarchical_tree.gml')\n",
    "labels_and_descriptions = pd.read_csv(f'{model_path}/labels_and_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>Perspective Alignment</td>\n",
       "      <td>The reasoning involves aligning chessboard coo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>Knight Movement</td>\n",
       "      <td>Focuses on calculating possible knight moves f...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>Invalid Move Check</td>\n",
       "      <td>Identifying and discarding impossible moves to...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129</td>\n",
       "      <td>Move Legitimacy</td>\n",
       "      <td>Checking board state to rule out impossible mo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132</td>\n",
       "      <td>Verification</td>\n",
       "      <td>Systematically checking FEN notation and move ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_id                  label  \\\n",
       "0      140  Perspective Alignment   \n",
       "1      133        Knight Movement   \n",
       "2      141     Invalid Move Check   \n",
       "3      129        Move Legitimacy   \n",
       "4      132           Verification   \n",
       "\n",
       "                                         description  level  \n",
       "0  The reasoning involves aligning chessboard coo...      6  \n",
       "1  Focuses on calculating possible knight moves f...      6  \n",
       "2  Identifying and discarding impossible moves to...      6  \n",
       "3  Checking board state to rule out impossible mo...      6  \n",
       "4  Systematically checking FEN notation and move ...      6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_and_descriptions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = (\n",
    "    pd.read_json('../experiments/reasoning/qwq-32b/make_hierarchy/qwq-32b-rollouts-labeled.json')\n",
    ")\n",
    "input_data = input_data.loc[lambda df: df['category'] == 'chess_puzzle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_path\n",
       "Nexusflow/QwQ-32B-GenFix    727\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['model_path'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a chess puzzle. The board state is r5rk/pppq1pb1/3p1N1R/5P2/4PQ2/6P1/PPP5/R4K2 b - - 0 33. Your opponent plays g7h6. What is the best next move? You answer should be in in algebraic notation. Box your answer with \\boxed{}.\n"
     ]
    }
   ],
   "source": [
    "sample_chess_problem = input_data['prompt'].iloc[0]\n",
    "print(sample_chess_problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run on Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format prompt\n",
    "FOLLOW_HIGH_LEVEL_PROMPT_START = \"\"\"\n",
    "You are a chess expert. Here is a problem you've been trying to solve:\n",
    "\n",
    "<problem>\n",
    "{problem}\n",
    "</problem>\n",
    "\n",
    "Think about how to solve this problem. Try to follow this approach as you think:\n",
    "\n",
    "<approach>\n",
    "{approach}\n",
    "</approach>\n",
    "\n",
    "Here are some examples of how to follow this approach:\n",
    "\n",
    "<approach_examples>\n",
    "{approach_examples}\n",
    "</approach_examples>\n",
    "\n",
    "Do NOT follow the examples exactly. Instead, use them as a guide to think about how to solve the problem.\n",
    "\n",
    "Output a SINGLE thought. This thought should be a SINGLE step in your approach to solving the problem, like the examples. Do not include any other thoughts or information. STOP after the thought.\n",
    "\n",
    "<thought>\n",
    "\"\"\"\n",
    "\n",
    "FOLLOW_HIGH_LEVEL_PROMPT_CONTINUATION = \"\"\"\n",
    "You are a chess expert. Here is a problem you've been trying to solve:\n",
    "\n",
    "<problem>\n",
    "{problem}\n",
    "</problem>\n",
    "\n",
    "You are thinking about how to solve this problem. Here is what you've thought so far:\n",
    "\n",
    "<thinking>\n",
    "{thinking}\n",
    "</thinking>\n",
    "\n",
    "Now, think about what to do next. Try to follow this approach:\n",
    "\n",
    "<approach>\n",
    "{approach}\n",
    "</approach>\n",
    "\n",
    "Here are some examples of how to follow this approach:\n",
    "\n",
    "<approach_examples>\n",
    "{approach_examples}\n",
    "</approach_examples>\n",
    "\n",
    "Do NOT follow the examples exactly. Instead, use them as a guide to think about how to solve the problem.\n",
    "\n",
    "Output a SINGLE thought. This thought should be a SINGLE step in your approach to solving the problem, like the examples. Do not include any other thoughts or information. STOP after the thought.\n",
    "\"\"\"\n",
    "\n",
    "FOLLOW_HIGH_LEVEL_PROMPT_FINAL = FOLLOW_HIGH_LEVEL_PROMPT_CONTINUATION + \"\"\"\n",
    "If you are finished thinking, output your answer enclosed in <answer> tags.\n",
    "\"\"\"\n",
    "\n",
    "CHOOSE_FIRST_NODE_PROMPT = \"\"\"\n",
    "You are a chess expert. Here is a problem you've been trying to solve:\n",
    "\n",
    "<problem>\n",
    "{problem}\n",
    "</problem>\n",
    "\n",
    "You will start thinking about how to solve this problem. Choose the best approach from below to start thinking about your problem:\n",
    "\n",
    "<options>\n",
    "{options}\n",
    "</options>\n",
    "\n",
    "What approach will you take first? Choose the best option from above. Your response:\n",
    "\"\"\"\n",
    "\n",
    "CHOOSE_NEXT_NODE_PROMPT = \"\"\"\n",
    "You are a chess expert. Here is a problem you've been trying to solve:\n",
    "\n",
    "<problem>\n",
    "{problem}\n",
    "</problem>\n",
    "\n",
    "You are thinking about how to solve this problem. Here is what you've thought so far:\n",
    "\n",
    "<thinking>\n",
    "{thinking}\n",
    "</thinking>\n",
    "\n",
    "You are thinking about what thoughts to have next. Here are some options:\n",
    "\n",
    "<options>\n",
    "{options}\n",
    "</options>\n",
    "\n",
    "What will you think next? Choose the best option from above. Your response:\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, create_model\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "class NextThought(BaseModel):\n",
    "  next_thought: str\n",
    "\n",
    "def make_labeling_structure(possible_labels: list[str]):\n",
    "    return create_model(\n",
    "        'NextThoughtType',\n",
    "        next_thought_type=(Literal[tuple(possible_labels)], ...) # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Prompts/Choose Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotated_types import Len\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "LABEL_LEVEL = 3\n",
    "examples_file = f\"{model_path}/labeled_chunks__level-{LABEL_LEVEL}.csv\"\n",
    "examples_df = pd.read_csv(examples_file, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHOOSE_BEST_EXAMPLES_PROMPT =\"\"\"You are a chess expert. \n",
    "\n",
    "I am trying to find good examples of thought patterns for a chess problem.\n",
    "\n",
    "Here is the thought pattern I am looking for:\n",
    "\n",
    "<thought_pattern>\n",
    "{thought_pattern}\n",
    "</thought_pattern>\n",
    "\n",
    "Here are some examples of thought patterns:\n",
    "\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Choose the best 5 examples from the list above.\n",
    "If the list contains less than 5 examples, choose all of them.\n",
    "You can rewrite the examples you choose to make them more specific or clearer if you think that is helpful, \n",
    "but don't change the meaning of the examples or add any new information. \n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "class BestExamples(BaseModel):\n",
    "  best_examples: Annotated[list[str], Len(min_length=5, max_length=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395039e786e94f37a6241660f8f4f889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best examples:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_examples = []\n",
    "for thought_pattern in tqdm(examples_df['labels'].unique().tolist(), desc='Finding best examples'):\n",
    "    examples = (\n",
    "         examples_df\n",
    "            .loc[lambda df: df['chunks'].str.split().str.len() < 400]\n",
    "            .loc[lambda df: df['chunks'].str.split().str.len() > 50]\n",
    "            .loc[lambda df: df['labels'] == thought_pattern, 'chunks']\n",
    "    )\n",
    "    examples = '\\n\\n'.join(examples.pipe(lambda df: df.sample(min(20, len(df)))).tolist())\n",
    "    label_description = (\n",
    "        labels_and_descriptions\n",
    "            .loc[lambda df: df['level'] == LABEL_LEVEL]\n",
    "            .loc[lambda df: df['label'] == thought_pattern]\n",
    "            .apply(lambda x: f\"\\\"{x['label']}\\\": {x['description']}\", axis=1)\n",
    "            .iloc[0]\n",
    "    )\n",
    "    best_examples_prompt = CHOOSE_BEST_EXAMPLES_PROMPT.format(thought_pattern=label_description, examples=examples)\n",
    "    r = prompt_openai_model(model_name='gpt-4o', prompt=best_examples_prompt, response_format=BestExamples)\n",
    "    best_examples.append({\n",
    "        'label': thought_pattern,\n",
    "        'examples': r.best_examples\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_row = pd.Series({'label': 'Finish thinking', 'description': 'Finish thinking about the problem and answer.'}).to_frame().T\n",
    "formatted_next_move_options = (\n",
    "    labels_and_descriptions\n",
    "        .loc[lambda df: df['level'] == LABEL_LEVEL]\n",
    "        .pipe(lambda df: pd.concat([df, finished_row]))\n",
    "        .assign(formatted_description=lambda df: df.apply(lambda x: f\"\\\"{x['label']}\\\": {x['description']}\", axis=1))\n",
    "        .sample(frac=1)\n",
    "        .merge(pd.DataFrame(best_examples), on='label', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_choices_format = make_labeling_structure(formatted_next_move_options['label'].tolist())\n",
    "label_choices_format_json = label_choices_format.model_json_schema()\n",
    "next_move_options = '\\n'.join(formatted_next_move_options['formatted_description'].sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5325af4a1d9435685c9795ef0a195f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thinking...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_thoughts = []\n",
    "thought_types = []\n",
    "\n",
    "# get first thought type\n",
    "starting_high_level_prompt = CHOOSE_FIRST_NODE_PROMPT.format(problem=sample_chess_problem, options=next_move_options)\n",
    "r = prompt_openai_model(model_name='gpt-4o-mini', prompt=starting_high_level_prompt, response_format=label_choices_format)\n",
    "next_thought_type = r.next_thought_type\n",
    "thought_types.append(next_thought_type)\n",
    "\n",
    "# get examples and formatted description for first thought type\n",
    "examples, formatted_desc = (\n",
    "    formatted_next_move_options.loc[lambda df: df['label'] == next_thought_type]\n",
    "    .iloc[0]\n",
    "    [['examples', 'formatted_description']]\n",
    ")\n",
    "starting_low_level_prompt = FOLLOW_HIGH_LEVEL_PROMPT_START.format(\n",
    "    problem=sample_chess_problem, \n",
    "    approach=formatted_desc, \n",
    "    approach_examples='\\n\\n'.join(examples)\n",
    ")\n",
    "thoughts = prompt_openai_model(model_name='gpt-4o-mini', prompt=starting_low_level_prompt)\n",
    "all_thoughts.append(thoughts)\n",
    "\n",
    "\n",
    "# now start iterating...\n",
    "# get next thought type\n",
    "for _ in tqdm(range(10), desc='Thinking...'):\n",
    "    thought_format = list(map(lambda x: f'{x[0]} thought type: {x[1]}', zip(thought_types, all_thoughts)))\n",
    "    next_thought_type_prompt = CHOOSE_NEXT_NODE_PROMPT.format(\n",
    "        problem=sample_chess_problem, \n",
    "        thinking='\\n'.join(thought_format), \n",
    "        options=next_move_options\n",
    "    )\n",
    "    r = prompt_openai_model(model_name='gpt-4o-mini', prompt=next_thought_type_prompt, response_format=label_choices_format)\n",
    "    next_thought_type = r.next_thought_type\n",
    "    thought_types.append(next_thought_type)\n",
    "\n",
    "    # get examples and formatted description for next thought type\n",
    "    examples, formatted_desc = (\n",
    "        formatted_next_move_options.loc[lambda df: df['label'] == next_thought_type]\n",
    "        .iloc[0]\n",
    "        [['examples', 'formatted_description']]\n",
    "    )\n",
    "    CONTINUATION_PROMPT = FOLLOW_HIGH_LEVEL_PROMPT_CONTINUATION if next_thought_type != 'Finish thinking' else FOLLOW_HIGH_LEVEL_PROMPT_FINAL\n",
    "    next_low_level_prompt = CONTINUATION_PROMPT.format(\n",
    "        problem=sample_chess_problem, \n",
    "        thinking=thoughts, \n",
    "        approach=formatted_desc, \n",
    "        approach_examples='\\n\\n'.join(examples) if next_thought_type != 'Finish thinking' else ''\n",
    "    )\n",
    "    thoughts = prompt_openai_model(model_name='gpt-4o-mini', prompt=next_low_level_prompt)\n",
    "    all_thoughts.append(thoughts)\n",
    "    if next_thought_type == 'Finish thinking':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I need to evaluate the position after my opponent plays g7h6, considering potential threats and opportunities for my pieces, particularly focusing on how I can utilize my rook on h7 to create a check or a strong attack against the black king.',\n",
       " 'Considering the move Qe7-g5, the white queen can move to g5, attacking the black pawn on h6 and threatening to create a check on the black king on h8. This move also puts pressure on the black pieces, forcing them to respond to the threat against the pawn and the potential check.',\n",
       " '<answer>\\\\boxed{Qg5}</answer>']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tactical Evaluation', 'Move Analysis', 'Finish thinking']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thought_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \n",
    "# FOLLOW_HIGH_LEVEL_PROMPT_CONTINUATION.format(problem=sample_chess_problem, thinking='', approach='', approach_examples='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Let's first examine the move g7h6 and its implications on the board. Perhaps this pawn push is an attempt to prepare for a potential ...h5 break, which could then be met with a pawn storm of my own. In that case, I should look out for possibilities like a kingside counterattack or creating threats against Black's pawns.\""
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': starting_low_level_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run on together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/Deepseek-R1-Distill-Qwen-1.5B\",\n",
    "    messages=[{\"role\": \"user\", \"content\": sample_chess_problem}],\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
